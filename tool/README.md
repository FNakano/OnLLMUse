# Run Python functions in a LLM session

## Motivation

I want to create a full log of what I chat with a LLM. A log has date information for each logged event. I want to get date/time information within a running LLM.  

## Introduction

In LLM vocabulary, a function external to the LLM which can be executed by the LLM is a *tool*.

A LLM, on its own, is not capable to run a shell command like `date` nor is capable to browse the web for a site that tells the weather in Paris. It needs *tools* to do it.

A LLM that can execute functions is a LLM Agent.

There is an Ollama tutorial for it: https://docs.ollama.com/capabilities/tool-calling#python. I tested, it did what I expected it to do.

```python
from ollama import chat

def get_temperature(city: str) -> str:
  """Get the current temperature for a city
  
  Args:
    city: The name of the city

  Returns:
    The current temperature for the city
  """
  temperatures = {
    "New York": "22Â°C",
    "London": "15Â°C",
    "Tokyo": "18Â°C",
  }
  return temperatures.get(city, "Unknown")

messages = [{"role": "user", "content": "What is the temperature in New York?"}]

# pass functions directly as tools in the tools list or as a JSON schema
response = chat(model="qwen3", messages=messages, tools=[get_temperature], think=True)

messages.append(response.message)
if response.message.tool_calls:
  # only recommended for models which only return a single tool call
  call = response.message.tool_calls[0]
  result = get_temperature(**call.function.arguments)
  # add the tool result to the messages
  messages.append({"role": "tool", "tool_name": call.function.name, "content": str(result)})

  final_response = chat(model="qwen3", messages=messages, tools=[get_temperature], think=True)
  print(final_response.message.content)
```

There is an explanation on how tool calling works in https://www.philschmid.de/gemma-function-calling

## Results

My final code is in `tooltest.py`. I added some `print` messages and some more items in the list to make sure I understood enough. Calling the script with `python3 -m tooltest.py` and waiting for some (three?) minutes resulted in:

<pre><font color="#8AE234"><b>fabio@super</b></font>:<font color="#729FCF"><b>~/MeuGithub/OnLLMUse/tool</b></font>$ python3 -m tooltest
function=Function(name=&apos;get_temperature&apos;, arguments={&apos;city&apos;: &apos;SÃ£o Paulo&apos;})
{&apos;city&apos;: &apos;SÃ£o Paulo&apos;}
SÃ£o Paulo
25
A temperatura atual em SÃ£o Paulo Ã© de **25Â°C**. ðŸ˜Š  
Precisa de mais alguma informaÃ§Ã£o?
<font color="#8AE234"><b>fabio@super</b></font>:<font color="#729FCF"><b>~/MeuGithub/OnLLMUse/tool</b></font>$ 
</pre>

## Explanation - why I say it works

Parts of `tooltest.py` are the same as the quickstart example (https://docs.ollama.com/quickstart#python). These parts are initial configuration and chatting instructions. Differences are in `tools` argument and in the test of `response.message.tool_call`. If this argument is not None then there is a function to be called. Since `get_temperature` is the only function, it is called. Function call arguments are passed as **kwargs. I added some messages to print the function object, the argument and the result to see the function being executed.

## Comments

Be careful! There are huge concerns on LLM Agents about privacy and security. I do not take any responsibility of anything you do.

I did a lot of web searching and code testing to get to the presented code. I ommitted that for the sake of clarity.

There is a chance the code stops working in a couple of months due to some underlying tool update (e.g. if Ollama API changes its way to pass the tool list to the chat, my code will probably break).

Not all LLM models support tool calling. I could not do it with gemma3.

qwen3 supports tools but takes more memory, takes longer to answer and makes the computer draw more power... 

This code is very sensitive to city name misspelling because tool is called only once. I tested it with "Sao Paulo" in the `temperatures` dictionary. The conversation generated argument "city":"SÃ£o Paulo" which made `get` function to return `unknown`...

Functions are NOT evaluated. Its name should be tested and a specific function call should be done. This is supported by this code snippet generated by Gemini:
  
```python
# In your Python code:
if call.function.name == 'get_weather':
    result = get_weather(**json.loads(call.function.arguments))

```

More tests are necessary. For example, what happens when a message without a tool is sent? Can some exception occur? Can a tool call be triggered without need? Can a tool call be triggered during *Thinking*? What if there is more than one function that can be executed?

